\documentclass{article}
\usepackage{amsmath,amsfonts,bm,fullpage,subcaption,graphicx,caption}
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\newcommand{\ProjMean}{{\widehat{\bm S}_{2}}}
\newcommand{\R}{{\mathbb{R}}}
\begin{document}

<<setup, include=FALSE>>=
library(rotations)
#source("../robustFunctions.R")
#Rcpp::sourceCpp('../robustCpp.cpp')

# set global chunk options
opts_chunk$set(fig.path='Figure/', fig.align='center', fig.show='hold',dev='png')
options(replace.assign=TRUE,width=50)
@

\begin{center}
\Large{\bf Outlier Detection for Circular and Spherical Data}
\end{center}
\normalsize
This is the literature I have found for outliers for circular, spherical and $SO(3)$ data.

\section{Circular Data}\label{sec:circle}

For data on the circle each observation is an angle $\theta_i$ which correspond to the vector $\bm P_i\in \R^2$ according to $\bm P_i'=(x_i,y_i,z_i)=(\cos\theta_i,\sin\theta_i)$.  Define $S_x=\sum_{i=1}^nx_i$ and $S_y=\sum_{i=1}^ny_i$ then the resultant length is given by $R$ satisfying $R^2=S_x^2+S_y^2+S_z^2$, which has direction $\hat\theta_n$.  Also, $\bar{R}_n=R/n$.

In \cite{collett1980}, four statistics to identify outliers in circular data are defined.  Only the von Mises distribution on the circle is considered in the simulation study, but three of the statistics are general and could be applied to any circular distribution. 

The $L$-statistic is the likelihood ratio test statistic that tests the null hypothesis that all observations are from a single von Mises($\mu,\kappa$) versus the alternative that one observation comes from a von Mises distribution with a different mean direction.  This is sometimes called the location-slippage alternative hypothesis.  The test-statistic in the context of the von Mises distribution for the observation $\theta_k$ is
\[
L=(R_k+1)\hat{\kappa}_k-\hat{\kappa}R-n\log\left[\frac{I_0(\hat{\kappa}_k)}{I_0(\hat{\kappa})}\right]
\]
where $R_k$ and $\hat\kappa_k$ are the estimated mean resultant lengths and concentration parameter with $\theta_k$ removed and $I_0(x)$ is the modified Bessel function of the first kind with order zero.

The $C$-Statistic is more general.  It is interpreted as the relative increase in the sample mean resultant length after omitting the queried value.  It is given by
\[
C=\text{max}_i\left\{\frac{\bar{R}_i-\bar{R}}{\bar{R}}\right\}=\frac{R_k-R}{R}
\]
where $\bar{R}$ is the sample mean resultant length and $\bar{R}_i$ is the mean resultant length with the $i$th observation removed.  

The $D$-statistic is the ratio of the arcs on either side of the observation.  That is, let $\theta_{(i)}$ denote the $i$th ordered observation and define
\[
T_i=\theta_{(i+1)}-\theta_{(i)}\hspace{1em}\text{ and }\hspace{1em}T_n=2\pi-\theta_{(n)}+\theta_{(i)}.
\]
 Also set $T_{0}=T_{n}$ then for $i=1,\dots,n$, $D_i=T_i/T_{i-1}$ is the ratio of the arc-lengths on either side of the $i$th observation.  Finally let $\theta_k$ be the observation that visually appears to be an outlier and define $D=\text{min}(D_k,D_{k}^{-1})$ then $D\in(0,1)$ and is unambiguous.

<<dstat, fig.width=4, fig.height=4, out.width='.4\\linewidth',echo=FALSE,eval=FALSE>>=
#This is my SO(3) interpretation of the test statistics proposed in Collett
#(1980) to identify "surprising values" on the circle

dstat<-function(Rs){
  Shat<-mean(Rs)
  n<-nrow(Rs)
  ds<-rep(0,n)
  for(i in 1:n){
    Rsi<-as.SO3(Rs[-i,])
    Shati<-mean(Rsi)
    ds[i]<-dist(Shat,Shati,method='intrinsic')
  }
  return(ds)
}

Rs<-ruars(20,rcayley,kappa=50)
plot(Rs,label_points=round(dstat(Rs),3))
@


<<dmat,include=FALSE,eval=FALSE>>=
#The D-statistic is the arc lengths between successive observations.  Unfortunatlely that means we need to be able to order our data somehow.  The spherical interpretation of this statistic is due to Fisher, Lewis and Willcox (1981) and it centers the data around the mean, orders the data according to their arc-lengths from that center and computes G_n=(pi-theta_n)/(theta_n-theta_{n-1})

resids<-dist(Rs,mean(Rs),method='intrinsic')
thetaN<-max(resids)
thetaNm1<-max(resids[-which.max(resids)])
Gn<-(thetaN)/(thetaN-thetaNm1)

# dmat<-function(Rs){
#   n<-nrow(Rs)
#   rmat<-matrix(0,n,n)
#   for(i in 1:(n-1)){
#     Ri<-as.SO3(Rs[i,])
#     
#     for(j in (i+1):n){
#       Rj<-as.SO3(Rs[j,])
#       rmat[i,j]<-dist(Ri,Rj,method='intrinsic')
#     }
#   }
#   return(rmat)
# }
# 
# dmat(Rs)
# plot(Rs,label_points=round(dmat(Rs)[,20],3))
# 
# #Order them by distance from S-hat
# resids<-dist(Rs,mean(Rs),method='intrinsic')
# Rs<-as.SO3(Rs[order(resids),])
# 
# plot(Rs,label_points=1:20,col=3)
# 
# n<-nrow(Rs)
# Ti<-rep(0,n-1)
# for(i in 1:(n-1)){
#   Ti[i]<-dist(as.SO3(Rs[i,]),as.SO3(Rs[(i+1),]))
# }
@

Finally, the $M$-statistic was originally suggested by \cite{mardia1975} and is similar in spirit to the $C$-statistic, namely
\[
M=\min_i\left(\frac{n-1-R_i}{n-R}\right)
\]
where $R$ is mean resultant length based on the entire sample of size $n$ and $R_i$ is the mean resultant length after the $i$th observation is deleted.  To put this on the same scale as the other statistics discussed in \cite{collett1980} consider $1-M$.

In a simulation study they determine that the statistics with distributions that rely on $\kappa$, namely $C$ and $D$, are the best choices.  Because $D$ is easier to compute it comes the most highly recommended.

The following is according to Chapter 12 of \cite{mardia2009} because I cannot get a physical copy of either artilce.   \cite{best1986} use the relationship between the Watson distribution on the sphere and the exponential distribution to test a discordant point as an outlier.  This is an application of the ideas of FLW to a different distribution and space. \cite{bagchi1990} use Bayesian methods to test for discordant points en bloc, given the number of discordent points is fixed and known.  

In terms of estimators that accomodate outliers,  \cite{wehrly1981} compare the circular mean and median based on traditional influence functions.  Since the influence functions for both estimators are bounded and are therefor B-robust estimators the authors claim the mean is preferred since it is also the MLE. \cite{ko1988} later claim the work of \cite{wehrly1981} (and \cite{watson1983} for that matter) is garbage because using traditional influence curves for bounded parameter spaces is meaningless.  \cite{lenth1981} considers an iteratively re-weighted M-estimator.  The main drawback from his estimator is its reliance on a good estimate of concentration $\kappa$.  \cite{ducharme1987} intorduce the {\it normalized spatial median}, which is shown to perform well in the presence of outliers.  \cite{barnett1994} summarize most of this material in Chapter 11.


\section{Spherical Data}\label{sec:sphere}

For data on the sphere each observation consists of an angle pair $(\theta_i,\phi_i)$ which correspond to the vector $\bm P_i\in \R^3$ according to $\bm P_i'=(x_i,y_i,z_i)=(\sin\theta_i\cos\phi_i,\sin\theta_i\sin\phi_i,\cos\theta_i)$.  Define $S_x=\sum_{i=1}^nx_i$, $S_y=\sum_{i=1}^ny_i$ and $S_z=\sum_{i=1}^nz_i$ then the resultant length is given by $R$ satisfying $R^2=S_x^2+S_y^2+S_z^2$, which has direction $(\hat\theta_n,\hat\phi_n)$.  Also, $\bar{R}_n=R/n$.


\cite{fisher1981} extend the statistics of \cite{collett1980} to spherical data and also add a few of their own.  Several outliers in a single sample are also considered.  They focus on Fisher's distribution on the sphere.

The spherical analog of Collett's $C$-statistic is
\[
C_n=\frac{\bar{R}_n^{(i)}-\bar{R}_n}{\bar{R}_n}.
\]

Using the approximations coth$(\kappa)\approx 1$ and $\log(\text{sinh}\kappa)\approx\kappa-\log 2$ the spherical analogs of Collett's $M$ (and for $\kappa>2.5$ Collett's $L$) is 
\[
E_n=(n-2)\frac{1+R_{n-1}^{(i)}-R_n}{n-1-R_{n-1}^{(i)}}.
\]
$E_n$ is based on a generalized likelihood-ratio test statistic testing the alternative location-slippage hypothesis.

Alternative, one can test the alternative hypothesis that the mean directions are the same, but the concentration parameters are different.  This leads to the generalized likelihood-ratio test-statistic
\[
D_n=(n-1)f(\bar{R}_{n-1}^{(i)})+f(\bar{R}_1^{(i)})-nf(\bar{R}_n)
\]
where $f(\bar{r})=\log\hat\kappa(\bar{r})+(\bar{r}-1)\hat\kappa(\bar{r})$ and $\kappa(\bar{r})$ is the concentration parameter estimate based on the sample mean resultant length $\bar{r}$.

Let $l_i$, $m_i$ and $n_i$ be the direction cosines for $(\theta_i,\phi_i)$, $1\leq i\leq n$ and let $l_i'$, $m_i'$ and $n_i'$ be the direction cosines after they've been centered around a pole.  Define $\theta_i'=\cos^{-1}(n_i')$ and $c_i=1-n_i'$ then order them $\theta_{(i)}'$ and $c_{(i)}$.  The gap test for an outlying observation can be based on
\[
G_n=\frac{\pi-\theta_{(n)}'}{\theta_{(n)}'-\theta_{(n-1)}'}.
\]

Under certain situations the $c_i$ values from the $G_n$ statistic have a nice limiting distribution.  Therefore, another test statistic for discordancy is the test statistic Ea 1 of  page 197 \cite{barnett1994} given by
\[
X_n=\frac{c_{(n)}}{\sum_{i=1}^nc_i}.
\]

Under the same assumptions of $X_n$, Barnett and Lewis (1994 page 198) propose the final estimator
\[
Y_n=\frac{c_{(n)}-c_{(n-1)}}{c_{(n)}}.
\]

In a simulation study they come to a conclusion similar to Collett (1980), in that the statistics with distributions that depend on $\kappa$ perform the best.  More specifically, $C_n$ is the best when $\kappa$ is known, $E_n$ is best when the data are concentrated ($\hat{\kappa}\geq 10$) and $X_n$ otherwise.

A special type of spherical data is axial data (undirected lines).  These are related to quaternions because for axial data there is no difference between $\bm X$ and $-\bm X$, as is true with quaternions.  In \cite{best1986} and Section 6.3.2(iii) of \cite{fisher1987}, the following test of discordance for the sample of undirected lines corresponding to rows in the $n\times 3$ matrix $\bm X$ is proposed.  Let $T_n=\bm X^\top\bm X$ with ordered eigenvalues $\hat\tau_1,\hat\tau_2,\hat\tau_3$ and $T_{n-1}$ is the same with the $i$th observation deleted.  If the statistic
\[
H_n=(n-2)\frac{1+\hat\tau_{3,n-1}-\hat\tau_3}{n-1-\hat\tau_{3,n-1}}.
\]
is large than the deleted observation is considered discordant.  More specifically, for large $\kappa$ and for $\bm X_i\in \R^p$ from the Bipolar Watson distribution, $H_n$ has a limiting $F_{p-1,(n-2)(p-1)}$ distribution.  This statistic is adapted to en bloc testing of outliers by \cite{figueiredo2005}.  In \cite{figueiredo2007} a likelihood ratio statistic for testing one or en bloc outliers is shown to be well behaved.  I think this will translate nicely into quaternions.

<<quatHn,include=FALSE,eval=FALSE>>=
Rs<-ruars(20,rcayley,kappa=50)
plot(Rs)
Qs<-Q4(Rs)

HnFun2<-function(Qs){
  Tn<-t(Qs)%*%Qs
  n<-nrow(Qs)
  tau4n<-svd(Tn)$d[1]
  tau4n1<-rep(0,n)
  
  for(i in 1:n){
    Qsi<-Qs[-i,]
    Tn1<-t(Qsi)%*%Qsi
    tau4n1[i]<-svd(Tn1)$d[1]
  }
  Hn<-(n-2)*(1+tau4n1-tau4n)/(n-1-tau4n1)
  return(Hn)
}

stats<-HnFun(Qs)
plot(Rs,label_points=round(stats,3))
plot(Rs,label_points=round(stats,3),col=2)
plot(Rs,label_points=round(stats,3),col=3)

QsOutlier<-as.Q4(rbind(Qs,Q4(c(1,0,0),pi/2)))
RsOutlier<-SO3(QsOutlier)
statsOutlier<-HnFun(QsOutlier)
plot(RsOutlier,label_points=round(statsOutlier,3))
plot(RsOutlier,label_points=round(statsOutlier,3),col=2)
plot(RsOutlier,label_points=round(statsOutlier,3),col=3)

@

For graphical methods see \cite{lewis1982}.  For testing several possible outliers at once, sometimes called {\it en bloc}, \cite{fisher1981} consider a generalized version of $E_n$. \cite{kimber1985} also looks at en bloc testing based on a statistic similar in spirit to $X_n$.  He claims it does better than the en bloc methods of FLW.   \cite{fisher1982} considers robust estimation of the concentration parameter $\kappa$.   \cite{rivest1989} looks at outliers in the spherical regression case.   \cite{best1986} and \cite{fisher1987} look more at axial data.

{\bf ADD TESTS FOR UNIFORMITY}


\section{$SO(3)$ Ideas}

I have not found a formal discussion or definition of outliers for rotations. \cite{fletcher2008} use the term `outlier' a lot in connection with rotation data, but it is never defined.  For their simulation study they simulate some data from a distribution with one central direction, then simulate the rest of the data from the same distribution with the same mean rotated through $90^\circ$.

After applying some of the statistics from circular and spherical cases to rotation data, there appears to be a connection between the statistic $H_n$ and Collett's $C$ statistic.  The $H_n$ statistic is a function of the difference in the largest eigenvalue of the matrix $T=\sum_{i=1}^n\bm q_i\bm q_i^\top$ when the $i$th observation has been removed.  Collett's $C$ statistic is the relative change in the mean resultant length when the $i$th observation has been removed.   The statistics $H_n$ and $C$ can be translated to rotation data most easily when we consider the quaternion representation.  This is because the projected mean in the quaternion case is the eigenvector associated with the largest eigenvalue of $T$.  

Thus, for a sample of rotations in quaternion form $\bm q_1,\dots,\bm q_n$ let $\hat{\bm q}_2$ be the projected mean, $H_n$ be the statistic as defined in Section \ref{sec:sphere} and define the rotation analog of the C statistic as 
\[
C^{(i)}=\|\hat{\bm q}_2-\hat{\bm q}^{(i)}\|=\sqrt{8[1-(\hat{\bm q}_2^\top\hat{\bm q}^{(i)})]}
\]
where $\hat{\bm q}_n$ is the projected mean for the full data set and $\hat{\bm q}^{(i)}$ is the projected mean when the $i$th observation has been deleted.  Figure \ref{fig:HnC} illustrate the relationship between these statistics in a concentrated data set (left) and a diffuse data set (right).  Both show a strong relationship between $C$ and $H_n$.

<<HnC,echo=FALSE,out.width=".5\\textwidth",fig.align='center',fig.show='hold',fig.height=5,fig.width=5,fig.cap="The $H_n$ and $C$ statistics for a sample from the Cayley distribution with $\\kappa=50$ (left) and $\\kappa=1$ (right).",eval=FALSE>>=
mains <- expression(paste(d[R](R[i], hat(S)[2])))
Rs<-ruars(25,rcayley,kappa=50)
Hn<-HnFun(Rs)
C<-MeanMove(Rs,method='intrinsic')
rs<-dist(Rs,mean(Rs),method='intrinsic')

qplot(Hn,C,xlab=expression(H[n]),size=I(3),colour=rs)+theme_bw()+
  scale_colour_gradient(mains)
ggsave("Figure/HnCConcentrated.pdf",height=5,width=6.5)

Rs2<-ruars(25,rcayley,kappa=1)
Hn2<-HnFun(Rs2)
C2<-MeanMove(Rs2)
rs2<-dist(Rs2,mean(Rs2),method='intrinsic')
cpi2<-rep("A",nrow(Rs))
cpi2[which(abs(rs2-pi/2)<.05)]<-"B"

qplot(Hn2,C2,ylab="C",xlab=expression(H[n]),size=I(3),colour=rs2)+theme_bw()+
  scale_colour_gradient(mains)
ggsave("Figure/HnCDiffuse.pdf",height=5,width=6.5)

qplot(Hn2,C2,ylab="C",xlab=expression(H[n]),size=I(3),colour=rs2,shape=cpi2)+theme_bw()+
  scale_colour_gradient(mains)
@

<<HnDetail,include=FALSE,eval=FALSE>>=
#I think Hn is punishing observations near pi/2 instead of near pi, similar to the idea of punishing them based on the influence function, sin(r), instead of distance from center, r
#WRONG.  Hn is doing exactly what we want (punishing observations far from the center correctly), the C statistic is what is doing the wrong thing (punishing observations near pi/2)

Rs<-ruars(50,rcayley,kappa=1)
Hn<-HnFun(Rs)
rs<-angle(Rs)
C<-MeanMove(Rs)
plot(Hn,rs)
plot(Hn,C)
plot(rs,C)
lines(sort(rs),sin(sort(rs))*.08)
@

\begin{figure}
\centering
\begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Figure/HnCConcentrated}
        \caption{$\kappa=50$}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Figure/HnCDiffuse}
        \caption{$\kappa=1$}
        \label{fig:HnDiff}
\end{subfigure}
\caption{The $H_n$ and $C$ statistics for a sample from the Cayley distribution with $\kappa=50$ (left) and $\kappa=1$ (right).  The points are colored based on each observations distance from the estimated mean, i.e.~color is determined by $d_R(\bm R_i,\hat{\bm q}_2$)}
\label{fig:HnC}
\end{figure}

It is clear from Figure \ref{fig:HnDiff} that the $C$ statistic cannot distinguish between observations that are on the extremes of the spectrum, i.e.~are very close to the center or a $90^\circ$ rotation away.  Put another way, observations that are roughly $\pi/2$ radians away from the estimated central direction $\ProjMean$ maximizes $C$, while observations very near or $\pi$ radians away from the estimated center have the same $C$ value.  This is exactly what the influence function does as well; recall from the intervals paper that the influence function of the projected mean is $IF_2(\bm R_i,F)\propto\sin(r_i)\bm U_i$.  The $H_n$ statistic, however, is closely related to the rotational distance between each observation and the estiamated center $\hat{\bm q}_2$.  It would be nice to show $C$ is maximized and that $H_1=1$ when $d_R(\bm R_i,\ProjMean)=\pi/2$. 

The question as to which statistic is appropriate comes down to: why do you care?  Large $C$ statistic (and also influence function) values will determine which observation will have the largest impact on the projected mean estimate of center.  The $H_n$ statistic will tell you which observation is most unlike the others, but not necessairly the one that will impact estimation.

The three assumptions being made when using a UARS model are $r_i$ is distributed according to some circular distribution centered at $0$, $\bm U_i$ is distributed uniformly on the unit sphere, $r_i$ and $\bm U_i$ are independent.  The following are ideas on how to test these assumptions given an unexpected pattern in the data.
\begin{enumerate}
\item $r_i\sim C(r|\kappa)$: Use one of the statistics from Section \ref{sec:circle}
\item $\bm U_i$ uniformly distributed on the sphere:  use one of the tests of uniformity available for spherical data from Section \ref{sec:sphere}.  We could also do a likelihood ratio test comparing the likelihood of observing these data assuming the UARS and PARS models, see  \cite{bingham2012}.  This will require some asymptotics and will likely be difficult since the PARS model cannot be written down in closed form.
\item $r_i$ indep. $\bm U_i$: nothing exists for this.  
\item Omnibus test ideas, i.e. $\bm R_i$ is weird for whatever reason:
\begin{enumerate}
\item Using the quaternion, apply the $H_n$ statistic using the fourth (largest) eigenvalue in place of the third.
\item Use multivariate methods on $\bm h_i\in\R^3$ that generates $\bm R_i=\exp[\bm{\Phi}(\bm h_i)]$
\item $E_n$ check changes in resultant length of sample, can we do the same with $\|\sum_i\bm R_i\|$ or det$(\sum_i \bm R_i)$
\end{enumerate}
\end{enumerate}

\bibliography{RobustRefs}

\end{document}