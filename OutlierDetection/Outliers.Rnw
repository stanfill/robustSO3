\documentclass{article}
\usepackage{amsmath,amsfonts,bm,fullpage}
\usepackage{natbib}
\newcommand{\ProjMean}{{\widehat{\bm S}_{2}}}
\newcommand{\R}{{\mathbb{R}}}
\begin{document}

<<setup, include=FALSE>>=
library(knitr)
library(rotations)
# set global chunk options
opts_chunk$set(fig.path='Figure/', fig.align='center', fig.show='hold',dev='png')
options(replace.assign=TRUE,width=50)
@

\begin{center}
\Large{\bf Outlier Detection for Circular and Spherical Data}
\end{center}
\normalsize
This is the literature I have found for outliers for circular, spherical and $SO(3)$ data.

\section{Circular Data}\label{sec:circle}

For data on the circle each observation is an angle $\theta_i$ which correspond to the vector $\bm P_i\in \R^2$ according to $\bm P_i'=(x_i,y_i,z_i)=(\cos\theta_i,\sin\theta_i)$.  Define $S_x=\sum_{i=1}^nx_i$ and $S_y=\sum_{i=1}^ny_i$ then the resultant length is given by $R$ satisfying $R^2=S_x^2+S_y^2+S_z^2$, which has direction $\hat\theta_n$.  Also, $\bar{R}_n=R/n$.

In \cite{collett1980}, four statistics to identify outliers in circular data are defined.  Only the von Mises distribution on the circle is considered in the simulation study, but three of the statistics are general and could be applied to any circular distribution. 

The $L$-statistic is the likelihood ratio test statistic that tests the null hypothesis that all observations are from a single von Mises($\mu,\kappa$) versus the alternative that one observation comes from a von Mises distribution with a different mean direction.  This is sometimes called the location-slippage alternative hypothesis.  The test-statistic in the context of the von Mises distribution for the observation $\theta_k$ is
\[
L=(R_k+1)\hat{\kappa}_k-\hat{\kappa}R-n\log\left[\frac{I_0(\hat{\kappa}_k)}{I_0(\hat{\kappa})}\right]
\]
where $R_k$ and $\hat\kappa_k$ are the estimated mean resultant lengths and concentration parameter with $\theta_k$ removed and $I_0(x)$ is the modified Bessel function of the first kind with order zero.

The $C$-Statistic is more general.  It is interpreted as the relative increase in the sample mean resultant length after omitting the queried value.  It is given by
\[
C=\text{max}_i\left\{\frac{\bar{R}_i-\bar{R}}{\bar{R}}\right\}=\frac{R_k-R}{R}
\]
where $\bar{R}$ is the sample mean resultant length and $\bar{R}_i$ is the mean resultant length with the $i$th observation removed.  

The $D$-statistic is the ratio of the arcs on either side of the observation.  That is, let $\theta_{(i)}$ denote the $i$th ordered observation and define
\[
T_i=\theta_{(i+1)}-\theta_{(i)}\hspace{1em}\text{ and }\hspace{1em}T_n=2\pi-\theta_{(n)}+\theta_{(i)}.
\]
 Also set $T_{0}=T_{n}$ then for $i=1,\dots,n$, $D_i=T_i/T_{i-1}$ is the ratio of the arc-lengths on either side of the $i$th observation.  Finally let $\theta_k$ be the observation that visually appears to be an outlier and define $D=\text{min}(D_k,D_{k}^{-1})$ then $D\in(0,1)$ and is unambiguous.

<<dstat, fig.width=4, fig.height=4, out.width='.4\\linewidth',echo=FALSE,eval=FALSE>>=
#This is my SO(3) interpretation of the test statistics proposed in Collett
#(1980) to identify "surprising values" on the circle

dstat<-function(Rs){
  Shat<-mean(Rs)
  n<-nrow(Rs)
  ds<-rep(0,n)
  for(i in 1:n){
    Rsi<-as.SO3(Rs[-i,])
    Shati<-mean(Rsi)
    ds[i]<-dist(Shat,Shati,method='intrinsic')
  }
  return(ds)
}

Rs<-ruars(20,rcayley,kappa=50)
plot(Rs,label_points=round(dstat(Rs),3))
@


<<dmat,include=FALSE,eval=FALSE>>=
#The D-statistic is the arc lengths between successive observations.  Unfortunatlely that means we need to be able to order our data somehow.  The spherical interpretation of this statistic is due to Fisher, Lewis and Willcox (1981) and it centers the data around the mean, orders the data according to their arc-lengths from that center and computes G_n=(pi-theta_n)/(theta_n-theta_{n-1})

resids<-dist(Rs,mean(Rs),method='intrinsic')
thetaN<-max(resids)
thetaNm1<-max(resids[-which.max(resids)])
Gn<-(thetaN)/(thetaN-thetaNm1)

# dmat<-function(Rs){
#   n<-nrow(Rs)
#   rmat<-matrix(0,n,n)
#   for(i in 1:(n-1)){
#     Ri<-as.SO3(Rs[i,])
#     
#     for(j in (i+1):n){
#       Rj<-as.SO3(Rs[j,])
#       rmat[i,j]<-dist(Ri,Rj,method='intrinsic')
#     }
#   }
#   return(rmat)
# }
# 
# dmat(Rs)
# plot(Rs,label_points=round(dmat(Rs)[,20],3))
# 
# #Order them by distance from S-hat
# resids<-dist(Rs,mean(Rs),method='intrinsic')
# Rs<-as.SO3(Rs[order(resids),])
# 
# plot(Rs,label_points=1:20,col=3)
# 
# n<-nrow(Rs)
# Ti<-rep(0,n-1)
# for(i in 1:(n-1)){
#   Ti[i]<-dist(as.SO3(Rs[i,]),as.SO3(Rs[(i+1),]))
# }
@

Finally, the $M$-statistic was originally suggested by Mardia (1975) \cite{mardia1975} and is similar in spirit to the $C$-statistic, namely
\[
M=\min_i\left(\frac{n-1-R_i}{n-R}\right)
\]
where $R$ is mean resultant length based on the entire sample of size $n$ and $R_i$ is the mean resultant length after the $i$th observation is deleted.  To put this on the same scale as the other statistics discussed in \cite{collett1980} consider $1-M$.

In a simulation study they determine that the statistics with distributions that rely on $\kappa$, namely $C$ and $D$, are the best choices.  Because $D$ is easier to compute it comes the most highly recommended.

The following is according to Chapter 12 of Mardia and Jupp (2009) \cite{mardia2009} because I cannot get a physical copy of either artilce.  Best \& Fisher (1986) \cite{best1986} use the relationship between the Watson distribution on the sphere and the exponential distribution to test a discordant point as an outlier.  This is an application of the ideas of FLW to a different distribution and space.  Bagchi \& Guttman (1990) \cite{bagchi1990} use Bayesian methods to test for discordant points en bloc, given the number of discordent points is fixed and known.  

In terms of estimators that accomodate outliers, Wehrly and Shine (1981) \cite{wehrly1981} compare the circular mean and median based on traditional influence functions.  Since the influence functions for both estimators are bounded and are therefor B-robust estimators the authors claim the mean is preferred since it is also the MLE.  Ko \& Guttorp (1988) \cite{ko1988} later claim the work of Wehrly and Shine (1981) (and Watson (1983) \cite{watson1983} for that matter) is garbage because using traditional influence curves for bounded parameter spaces is meaningless.  Lenth (1981) \cite{lenth1981} considers an iteratively re-weighted M-estimator.  The main drawback from his estimator is its reliance on a good estimate of concentration $\kappa$.  Ducharme \& Milasevic (1987) \cite{ducharme1987} intorduce the {\it normalized spatial median}, which is shown to perform well in the presence of outliers.  Barnett \& Lewis \cite{barnett1994} summarize most of this material in Chapter 11.


\section{Spherical Data}\label{sec:sphere}

For data on the sphere each observation consists of an angle pair $(\theta_i,\phi_i)$ which correspond to the vector $\bm P_i\in \R^3$ according to $\bm P_i'=(x_i,y_i,z_i)=(\sin\theta_i\cos\phi_i,\sin\theta_i\sin\phi_i,\cos\theta_i)$.  Define $S_x=\sum_{i=1}^nx_i$, $S_y=\sum_{i=1}^ny_i$ and $S_z=\sum_{i=1}^nz_i$ then the resultant length is given by $R$ satisfying $R^2=S_x^2+S_y^2+S_z^2$, which has direction $(\hat\theta_n,\hat\phi_n)$.  Also, $\bar{R}_n=R/n$.


Fisher, Lewis \& Willcox (1981) \cite{fisher1981} extend the statistics of \cite{collett1980} to spherical data and also add a few of their own.  Several outliers in a single sample are also considered.  They focus on Fisher's distribution on the sphere.

The spherical analog of Collett's $C$-statistic is
\[
C_n=\frac{\bar{R}_n^{(i)}-\bar{R}_n}{\bar{R}_n}.
\]

Using the approximations coth$(\kappa)\approx 1$ and $\log(\text{sinh}\kappa)\approx\kappa-\log 2$ the spherical analogs of Collett's $M$ (and for $\kappa>2.5$ Collett's $L$) is 
\[
E_n=(n-2)\frac{1+R_{n-1}^{(i)}-R_n}{n-1-R_{n-1}^{(i)}}.
\]
$E_n$ is based on a generalized likelihood-ratio test statistic testing the alternative location-slippage hypothesis.

Alternative, one can test the alternative hypothesis that the mean directions are the same, but the concentration parameters are different.  This leads to the generalized likelihood-ratio test-statistic
\[
D_n=(n-1)f(\bar{R}_{n-1}^{(i)})+f(\bar{R}_1^{(i)})-nf(\bar{R}_n)
\]
where $f(\bar{r})=\log\hat\kappa(\bar{r})+(\bar{r}-1)\hat\kappa(\bar{r})$ and $\kappa(\bar{r})$ is the concentration parameter estimate based on the sample mean resultant length $\bar{r}$.

Let $l_i$, $m_i$ and $n_i$ be the direction cosines for $(\theta_i,\phi_i)$, $1\leq i\leq n$ and let $l_i'$, $m_i'$ and $n_i'$ be the direction cosines after they've been centered around a pole.  Define $\theta_i'=\cos^{-1}(n_i')$ and $c_i=1-n_i'$ then order them $\theta_{(i)}'$ and $c_{(i)}$.  The gap test for an outlying observation can be based on
\[
G_n=\frac{\pi-\theta_{(n)}'}{\theta_{(n)}'-\theta_{(n-1)}'}.
\]

Under certain situations the $c_i$ values from the $G_n$ statistic have a nice limiting distribution.  Therefore, another test statistic for discordancy is the test statistic Ea 1 of Barnett and Lewis (1994, page 197) \cite{barnett1994} given by
\[
X_n=\frac{c_{(n)}}{\sum_{i=1}^nc_i}.
\]

Under the same assumptions of $X_n$, Barnett and Lewis (1994 page 198) propose the final estimator
\[
Y_n=\frac{c_{(n)}-c_{(n-1)}}{c_{(n)}}.
\]

In a simulation study they come to a conclusion similar to Collett (1980), in that the statistics with distributions that depend on $\kappa$ perform the best.  More specifically, $C_n$ is the best when $\kappa$ is known, $E_n$ is best when the data are concentrated ($\hat{\kappa}\geq 10$) and $X_n$ otherwise.

A special type of spherical data is axial data (undirected lines).  These are related to quaternions because for axial data there is no difference between $\bm X$ and $-\bm X$, as is true with quaternions.  In Best and Fisher (1986) \cite{best1986} and Section 6.3.2(iii) Fisher et al. (1987) \cite{fisher1987} proposed the following test of discordance for the sample of undirected lines corresponding to rows in the $n\times 3$ matrix $\bm X$.  Let $T_n=\bm X'\bm X$ with ordered eigenvalues $\hat\tau_1,\hat\tau_2,\hat\tau_3$ and $T_{n-1}$ is the same with the $n$th observation deleted.  If the statistic
\[
H_n=(n-2)\frac{1+\hat\tau_{3,n-1}-\hat\tau_3}{n-1-\hat\tau_{3,n-1}}.
\]
is large than the deleted observation is considered discordant.  More specifically, for large $\kappa$ and for $\bm X_i\in \R^p$ from the Bipolar Watson distribution, $H_n$ has a limiting $F_{p-1,(n-2)(p-1)}$ distribution.  This statistic is adapted to en bloc testing of outliers by Figueiredo \& Gomes (2005) \cite{figueiredo2005}.  In Figueiredo (2007) \cite{figueiredo2007} a likelihood ratio statistic for testing one or en bloc outliers is shown to be well behaved.  I think this will translate nicely into quaternions.

<<quatHn,include=FALSE,eval=FALSE>>=
Rs<-ruars(20,rcayley,kappa=50)
plot(Rs)
Qs<-Q4(Rs)

HnFun<-function(Qs){
  Tn<-t(Qs)%*%Qs
  n<-nrow(Qs)
  tau4n<-svd(Tn)$d[1]
  tau4n1<-rep(0,n)
  
  for(i in 1:n){
    Qsi<-Qs[-i,]
    Tn1<-t(Qsi)%*%Qsi
    tau4n1[i]<-svd(Tn1)$d[1]
  }
  Hn<-(n-2)*(1+tau4n1-tau4n)/(n-1-tau4n1)
  return(Hn)
}

stats<-HnFun(Qs)
plot(Rs,label_points=round(stats,3))
plot(Rs,label_points=round(stats,3),col=2)
plot(Rs,label_points=round(stats,3),col=3)

QsOutlier<-as.Q4(rbind(Qs,Q4(c(1,0,0),pi/2)))
RsOutlier<-SO3(QsOutlier)
statsOutlier<-HnFun(QsOutlier)
plot(RsOutlier,label_points=round(statsOutlier,3))
plot(RsOutlier,label_points=round(statsOutlier,3),col=2)
plot(RsOutlier,label_points=round(statsOutlier,3),col=3)

@

For graphical methods see Lewis and Fisher (1982) \cite{lewis1982}.  For testing several possible outliers at once, sometimes called {\it en bloc}, FLW \cite{fisher1981} consider a generalized version of $E_n$.  Kimber (1985) \cite{kimber1985} also looks at en bloc testing based on a statistic similar in spirit to $X_n$.  He claims it does better than the en bloc methods of FLW.  Fisher (1982) \cite{fisher1982} considers robust estimation of the concentration parameter $\kappa$.  Rivest (1989) \cite{rivest1989} looks at outliers in the spherical regression case.  Best \& Fisher (1986) \cite{best1986} and Fisher, Lewis \& Embleton (1993) \cite{fisher1987} look more at axial data.

{\bf ADD TESTS FOR UNIFORMITY}


\section{$SO(3)$ Ideas}

I have not found a formal discussion or definition of rotations. Fletcher et al. (2008) \cite{fletcher2008} use the term `outlier' a lot in connection with rotation data, but it is never defined.  For their simulation study they simulate some data from a distribution with one central direction, then simulate the rest of the data from the same distribution with a mean rotated through $90^\circ$.

The three assumptions being made when using a UARS model are $r_i$ is distributed according to some circular distribution centered at $0$, $\bm U_i$ is distributed uniformly on the unit sphere, $r_i$ and $\bm U_i$ are independent.  The following are ideas on how to test these assumptions given an unexpected pattern in the data.
\begin{enumerate}
\item $r_i\sim C(r|\kappa)$: Use one of the statistics from Section \ref{sec:circle}
\item $\bm U_i$ uniformly distributed on the sphere:  use one of the tests of uniformity available for spherical data from Section \ref{sec:sphere}.  We could also do a likelihood ratio test comparing the likelihood of observing these data assuming the UARS and PARS models, see Bingham et al. (2012) \cite{bingham2012}.  This will require some asymptotics and will likely be difficult since the PARS model cannot be written down in closed form.
\item $r_i$ indep. $\bm U_i$: nothing exists for this.  
\item Omnibus test ideas, i.e. $\bm R_i$ is weird for whatever reason:
\begin{enumerate}
\item Using the quaternion, apply the $H_n$ statistic using the fourth (largest) eigenvalue in place of the third.
\item Use multivariate methods on $\bm h_i\in\R^3$ that generates $\bm R_i=\exp[\bm{\Phi}(\bm h_i)]$
\item $E_n$ check changes in resultant length of sample, can we do the same with $\|\sum_i\bm R_i\|$ or det$(\sum_i \bm R_i)$
\end{enumerate}
\end{enumerate}

\bibliographystyle{plain}
\bibliography{RobustRefs}

\end{document}