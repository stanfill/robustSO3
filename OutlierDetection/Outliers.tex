\documentclass{article}\usepackage{graphicx, color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.2, 0.2, 0.2}
\newcommand{\hlnumber}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlfunctioncall}[1]{\textcolor[rgb]{0.501960784313725,0,0.329411764705882}{\textbf{#1}}}%
\newcommand{\hlstring}[1]{\textcolor[rgb]{0.6,0.6,1}{#1}}%
\newcommand{\hlkeyword}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlargument}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlcomment}[1]{\textcolor[rgb]{0.180392156862745,0.6,0.341176470588235}{#1}}%
\newcommand{\hlroxygencomment}[1]{\textcolor[rgb]{0.43921568627451,0.47843137254902,0.701960784313725}{#1}}%
\newcommand{\hlformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hleqformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlassignement}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlpackage}[1]{\textcolor[rgb]{0.588235294117647,0.709803921568627,0.145098039215686}{#1}}%
\newcommand{\hlslot}[1]{\textit{#1}}%
\newcommand{\hlsymbol}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlprompt}[1]{\textcolor[rgb]{0.2,0.2,0.2}{#1}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{amsmath,amsfonts,bm,fullpage}
\usepackage{natbib}
\newcommand{\ProjMean}{{\widehat{\bm S}_{2}}}
\newcommand{\R}{{\mathbb{R}}}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}




\begin{center}
\Large{\bf Outlier Detection for Circular and Spherical Data}
\end{center}
\normalsize
In this document I summarize the work done for outlier detection for circular, spherical and related data types

\section{Circular Data}

In \cite{collett1980}, four statistics to identify outliers in circular data are defined.  Only the von Mises distribution on the circle is considered in the simulation study, but three of the statistics are general and could be applied to any circular distribution. 

The $L$-statistic is the likelihood ratio test statistic that tests the null hypothesis that all observations are from a single von Mises($\mu,\kappa$) versus the alternative that one observation comes from a von Mises distribution with a different mean direction.  This is sometimes called the location-slippage alternative hypothesis.  The test-statistic in the context of the von Mises distribution for the observation $\theta_k$ is
\[
L=(R_k+1)\hat{\kappa}_k-\hat{\kappa}R-n\log\left[\frac{I_0(\hat{\kappa}_k)}{I_0(\hat{\kappa})}\right]
\]
where $R_k$ and $\hat\kappa_k$ are the estimated mean resultant lengths and concentration parameter with $\theta_k$ removed and $I_0(x)$ is the modified Bessel function of the first kind with order zero.

The $C$-Statistic is more general.  It is interpreted as the relative increase in the sample mean resultant length after omitting the queried value.  It is given by
\[
C=\text{max}_i\left\{\frac{\bar{R}_i-\bar{R}}{\bar{R}}\right\}=\frac{R_k-R}{R}
\]
where $\bar{R}$ is the sample mean resultant length and $\bar{R}_i$ is the mean resultant length with the $i$th observation removed.  

The $D$-statistic is the ratio of the arcs on either side of the observation.  That is, let $\theta_{(i)}$ denote the $i$th ordered observation and define
\[
T_i=\theta_{(i+1)}-\theta_{(i)}\hspace{1em}\text{ and }\hspace{1em}T_n=2\pi-\theta_{(n)}+\theta_{(i)}.
\]
 Also set $T_{0}=T_{n}$ then for $i=1,\dots,n$, $D_i=T_i/T_{i-1}$ is the ratio of the arc-lengths on either side of the $i$th observation.  Finally let $\theta_k$ be the observation that visually appears to be an outlier and define $D=\text{min}(D_k,D_{k}^{-1})$ then $D\in(0,1)$ and is unambiguous.








Finally, the $M$-statistic was originally suggested by Mardia (1975) \cite{mardia1975} and is similar in spirit to the $C$-statistic, namely
\[
M=\min_i\left(\frac{n-1-R_i}{n-R}\right)
\]
where $R$ is mean resultant length based on the entire sample of size $n$ and $R_i$ is the mean resultant length after the $i$th observation is deleted.  To put this on the same scale as the other statistics discussed in \cite{collett1980} consider $1-M$.

In a simulation study they determine that the statistics with distributions that rely on $\kappa$, namely $C$ and $D$, are the best choices.  Because $D$ is easier to compute it comes the most highly recommended.

According to Mardia et al. (2009) \cite{mardia2009},  Best \& Fisher (1986) \cite{best1986} and Bagchi \& Guttman \cite{bagchi1990} consider other ways to identify outliers but I can't get a hold of either of these articles to read.

\section{Spherical Data}

For data on the sphere each observation consists of an angle pair $(\theta_i,\phi_i)$ which correspond to the vector $\bm P_i\in \R^3$ according to $\bm P_i'=(x_i,y_i,z_i)=(\sin\theta_i\cos\phi_i,\sin\theta_i\sin\phi_i,\cos\theta_i)$.  Define $S_x=\sum_{i=1}^nx_i$, $S_y=\sum_{i=1}^ny_i$ and $S_z=\sum_{i=1}^nz_i$ then the resultant length is given by $R^2=S_x^2+S_y^2+S_z^2$, which has direction $(\hat\theta_n,\hat\phi_n)$.

Fisher, Lewis \& Willcox (1981) \cite{fisher1981} extend the statistics of \cite{collett1980} to spherical data and also add a few of their own.  Several outliers in a single sample are also considered.  They focus on Fisher's distribution on the sphere.

The spherical analog of Collett's $C$-statistic is
\[
C_n=\frac{\bar{R}_n^{(i)}-\bar{R}_n}{\bar{R}_n}.
\]

Using the approximations coth$(\kappa)\approx 1$ and $\log(\text{sinh}\kappa)\approx\kappa-\log 2$ the spherical analogs of Collett's $M$ (and for $\kappa>2.5$ Collett's $L$) is 
\[
E_n=(n-2)\frac{1+R_{n-1}^{(i)}-R_n}{n-1-R_{n-1}^{(i)}}.
\]
$E_n$ is based on a generalised likelihood-ratio test statistic testing the alternative location-slippage hypothesis.

Alternative, one can test the alternative hypothesis that the mean directions are the same, but the concentration parameters are different.  This leads to the generalised likelihood-ratio test-statistic
\[
D_n=(n-1)f(\bar{R}_{n-1}^{(i)})+f(\bar{R}_1^{(i)})-nf(\bar{R}_n)
\]
where $f(\bar{r})=\log\hat\kappa(\bar{r})+(\bar{r}-1)\hat\kappa(\bar{r})$ and $\kappa(\bar{r})$ is the concentration parameter estiamte based on the sample mean resultant length $\bar{r}$.

Let $l_i$, $m_i$ and $n_i$ be the direction cosines for $(\theta_i,\phi_i)$, $1\leq i\leq n$ and let $l_i'$, $m_i'$ and $n_i'$ be the direction cosines after they've been centered around a pole.  Define $\theta_i'=\cos^{-1}(n_i')$ and $c_i=1-n_i'$ then order them $\theat_{(i)}'$ and $c_{(i)}$.  The gap test for an outlying observation can be based on
\[
G_n=\frac{\pi-\theta_{(n)}'}{\theta_{(n)}'-\theta_{(n-1)}'}.
\]

Under certain situations the $c_i$ values from the $G_n$ statistic have a nice limiting distribuition.  Therefore, another test statistic for discordancy is the test statistic Ea 1 of Barnett and Lewis (1994, page 197) \cite{barnett1994} given by
\[
X_n=\frac{c_{(n)}}{\sum_{i=1}^nc_i}.
\]

Under the same assumptions of $X_n$, Barnett and Lewis (1994 page 1998) propse the final estimator
\[
Y_n=\frac{c_{(n)}-c_{(n-1)}}{c_{(n)}}.
\]

In a simulation study they come to a conslusion similar to Collett (1980), in that the statistics with distributions that depend on $\kappa$ perform the best.  More specifically, $C_n$ is the best when $\kappa$ is known, $E_n$ is best when the data are concentrated ($\hat{\kappa}\geq 10$) and $X_n$ otherwise.

\bibliographystyle{plain}
\bibliography{RobustRefs}

\end{document}
